{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d561b368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search for Aracaju\n",
      "\n",
      "\n",
      "\n",
      "N Days = 3 Range = (1, 4) Features = (1, 3) Epsilon = auto\n",
      "[==============================] 100.0% ... Total time: 0:00:09\n",
      " count: 0 mean acc: nan max acc: 0.6643636363636364\n",
      "N Days = 3 Range = (4, 7) Features = (1, 3) Epsilon = auto\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from scipy.signal import find_peaks_cwt \n",
    "from scipy.signal import find_peaks\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "import timeit\n",
    "import csv\n",
    "import time\n",
    "from scipy.stats import entropy\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "######################################\n",
    "# Begin of Utils\n",
    "######################################\n",
    "\n",
    "def gaussian_kernel(n):\n",
    "    x = np.linspace(-n, n, 2*n + 1)\n",
    "    sigma = (2 * n + 1) / 6;\n",
    "    y = 1 / (np.sqrt(np.pi) * sigma) * np.exp(-((x / sigma) ** 2) / 2)\n",
    "    y = y / np.sum(y)\n",
    "    return y\n",
    "\n",
    "def process_data(temp, precip, base, n):\n",
    "    nrow, ncol = temp.shape #Year, Days of Year\n",
    "    filt_t = np.zeros((nrow, n, ncol))\n",
    "    filt_p = np.zeros((nrow, n, ncol))\n",
    "\n",
    "    old_t = np.copy(temp);\n",
    "    old_p = np.copy(precip);\n",
    "    for i in range(0, n-1): #Power\n",
    "        j = int(np.round(base**(i+1)))\n",
    "        kg = gaussian_kernel(j)\n",
    "        displ = max(((2*j+1 - ncol)//2,0))\n",
    "        for l in range(0, nrow): # Year\n",
    "            c_t = np.convolve(temp[l,:], kg, mode='same')[displ:(displ+ncol)]\n",
    "            \n",
    "            #idx = find_peaks(precip[l,:])[0]\n",
    "            #c_p = np.zeros(ncol)\n",
    "            #c_p[idx] = 1\n",
    "            #c_p = np.convolve(c_p, kb, mode='same')\n",
    "            c_p = np.convolve(precip[l,:], kg, mode='same')[displ:(displ+ncol)]\n",
    "            \n",
    "\n",
    "            filt_t[l, i, :] = old_t[l,:] - c_t\n",
    "            filt_p[l, i, :] = old_t[l,:] - c_p\n",
    "        \n",
    "            old_t[l,:] = c_t\n",
    "            old_p[l,:] = c_p\n",
    "    for l in range(0, nrow):\n",
    "        filt_t[l, n-1, :] = old_t[l,:]\n",
    "        filt_p[l, n-1, :] = old_t[l,:]\n",
    "        \n",
    "    return filt_t, filt_p\n",
    "\n",
    "def my_train_test_split(x_temp, x_precip, y_dengue, idx_train, idx_test):\n",
    "    return x_temp[idx_train, :,:], x_temp[idx_test,:,:], x_precip[idx_train,:,:], x_precip[idx_test,:,:], y_dengue[idx_train], y_dengue[idx_test]\n",
    "\n",
    "def pack_data(x_temp, x_precip, start_date, n_days, l):\n",
    "    year, length, days = x_temp.shape\n",
    "    h_size = n_days * (l[1] - l[0])\n",
    "    v_size = 2 * h_size\n",
    "    \n",
    "    x_data = np.zeros((year, v_size))\n",
    "    \n",
    "    ia = l[0]\n",
    "    ib = l[1]\n",
    "    \n",
    "    ja = min(start_date, days)\n",
    "    jb = min(start_date + n_days, days)\n",
    "    \n",
    "    size = min((ib-ia)*(jb-ja), h_size)\n",
    "    x_data[:, 0:size] = x_temp[:, ia:ib, ja:jb].reshape(year, -1)\n",
    "    \n",
    "    size = min((ib-ia)*(jb-ja) + h_size, v_size)\n",
    "    x_data[:, h_size:size] = x_precip[:, ia:ib, ja:jb].reshape(year, -1)\n",
    "    return x_data\n",
    "\n",
    "\n",
    "def make_grid(x_t_train, x_t_val, x_t_test, x_p_train, x_p_val, x_p_test, y_d_train, y_d_val, y_d_test, n_days, l, model):\n",
    "    \n",
    "    years, lengths, days = x_t_test.shape\n",
    "    \n",
    "    m_train = {}\n",
    "    m_val = {}\n",
    "    m_test = {}\n",
    "    \n",
    "    m_train['accuracy'] = np.zeros(days - n_days)\n",
    "    m_val['accuracy'] = np.zeros(days - n_days)\n",
    "    m_test['accuracy'] = np.zeros(days - n_days)\n",
    "    \n",
    "    m_train['f1'] = np.zeros(days - n_days)\n",
    "    m_val['f1'] = np.zeros(days - n_days)\n",
    "    m_test['f1'] = np.zeros(days - n_days)\n",
    "    \n",
    "    m_train['recall'] = np.zeros(days - n_days)\n",
    "    m_val['recall'] = np.zeros(days - n_days)\n",
    "    m_test['recall'] = np.zeros(days - n_days)\n",
    "    \n",
    "    m_train['precision'] = np.zeros(days - n_days)\n",
    "    m_val['precision'] = np.zeros(days - n_days)\n",
    "    m_test['precision'] = np.zeros(days - n_days)\n",
    "\n",
    "    for start in range(0, days - n_days):\n",
    "        X_train = pack_data(x_t_train, x_p_train, start, n_days, l)\n",
    "        Y_train = y_d_train\n",
    "        \n",
    "        X_val = pack_data(x_t_val, x_p_val, start, n_days, l)\n",
    "        Y_val = y_d_val\n",
    "\n",
    "        X_test = pack_data(x_t_test, x_p_test, start, n_days, l)\n",
    "        Y_test= y_d_test\n",
    "\n",
    "        model.fit(X_train, Y_train) \n",
    "\n",
    "        y_p_train = model.predict(X_train)\n",
    "        y_p_val = model.predict(X_val)\n",
    "        y_p_test = model.predict(X_test)\n",
    "        \n",
    "        m_train['accuracy'][start] = accuracy_score(Y_train, y_p_train)\n",
    "        m_val['accuracy'][start] = accuracy_score(Y_val, y_p_val)\n",
    "        m_test['accuracy'][start] = accuracy_score(Y_test, y_p_test)\n",
    "        \n",
    "        m_train['f1'][start] = f1_score(Y_train, y_p_train)\n",
    "        m_val['f1'][start] = f1_score(Y_val, y_p_val)\n",
    "        m_test['f1'][start] = f1_score(Y_test, y_p_test)\n",
    "        \n",
    "        m_train['recall'][start] = recall_score(Y_train, y_p_train)\n",
    "        m_val['recall'][start] = recall_score(Y_val, y_p_val)\n",
    "        m_test['recall'][start] = recall_score(Y_test, y_p_test)\n",
    "        \n",
    "        m_train['precision'][start] = precision_score(Y_train, y_p_train)\n",
    "        m_val['precision'][start] = precision_score(Y_val, y_p_val)\n",
    "        m_test['precision'][start] = precision_score(Y_test, y_p_test)\n",
    "        \n",
    "    return m_test, m_val, m_train\n",
    "\n",
    "def saveMatrix(name, mat):\n",
    "    df = pd.DataFrame(data=mat.astype(float))\n",
    "    df.to_csv(name+'.csv', sep=' ', header=False, float_format='%.4f', index=False)\n",
    "\n",
    "def progress(count, total, suffix=''):\n",
    "    bar_len = 30\n",
    "    filled_len = int(round(bar_len * count / float(total+sys.float_info.epsilon)))\n",
    "\n",
    "    percents = round(100.0 * count / float(total+sys.float_info.epsilon), 1)\n",
    "    bar = '=' * filled_len + '-' * (bar_len - filled_len)\n",
    "\n",
    "    sys.stdout.write('[%s] %s%s ...%s\\r' % (bar, percents, '%', suffix))\n",
    "    sys.stdout.flush()  # As suggested by Rom Ruben\n",
    "    \n",
    "def formatTime(t):\n",
    "    s = t\n",
    "    m = int(s/60)\n",
    "    h = int(m/60)\n",
    "    s = int(math.fmod(s, 60))\n",
    "    m = int(math.fmod(m, 60))\n",
    "    return str(h)+\":\"+str(m).zfill(2)+\":\"+str(s).zfill(2)\n",
    "\n",
    "def formatPlot(ax, start=-5, num=9, shift=0):\n",
    "    ax.xaxis.set_ticks_position(\"bottom\")\n",
    "    \n",
    "    ax.set_xlabel('$t_0$', fontsize=20)\n",
    "    ax.set_ylabel('$p$', fontsize=20)\n",
    "    yticks=np.linspace(0, max_d-min_d,num=(max_d-min_d)//10+1, dtype=int)\n",
    "    ytlabels=yticks+10\n",
    "    ax.set_yticks(yticks)\n",
    "    ax.set_yticklabels(ytlabels)\n",
    "    \n",
    "    days=(31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31)\n",
    "    month=(\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\")\n",
    "    \n",
    "    xticks=np.roll(days, start+1)\n",
    "    xticks[0]=shift\n",
    "    xticks=np.cumsum(xticks)\n",
    "    xtlabels = np.roll(month, start);\n",
    "    ax.set_xticks(xticks[0:num])\n",
    "    ax.set_xticklabels(xtlabels[0:num], fontsize=12)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Diffusion Maps class scikit-learn style\n",
    "\n",
    "eps - floating point value, default=None(automatic)\n",
    "      Epsilon power to which the distance matrix is raised. \n",
    "      It controls the ammount of connections between points\n",
    "      \n",
    "features - tuple (a,b) int values, default=None(use all features)\n",
    "           Selects which range of features are returned when tranforming the data. \n",
    "           When None is set, return all available features.\n",
    "           Given data with n samples and m features, ie, X(n,m), the maximun\n",
    "           number of available features is **n**(yes n 'samples' not m 'features'\n",
    "           for this transform)\n",
    "\"\"\"\n",
    "class DiffusionMaps(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, eps=None, features=None, target_std=0.1, min_eps=0.001, max_eps=1000):\n",
    "        self._eps = eps\n",
    "        if(features is not None):\n",
    "            features = [i for i in range(features[0], features[1])]\n",
    "            \n",
    "        self._features = features\n",
    "        self._target_std = target_std\n",
    "        self._min_eps = min_eps\n",
    "        self._max_eps = max_eps\n",
    "        \n",
    "    def fit(self, X, y = None ):\n",
    "        self.X = X.copy()\n",
    "        D = cdist(X, X, 'sqeuclidean')\n",
    "        self.D=D\n",
    "        if(self._eps is  None):\n",
    "            self._eps = self.__find_eps()\n",
    "            \n",
    "        E = np.exp(-D / (self._eps**2))\n",
    "        self.P = normalize(E, axis=1, norm='l1')     \n",
    "        e, V = np.linalg.eig(self.P)\n",
    "        idx = np.argsort(e.real)\n",
    "        e = e.real[idx[::-1]]\n",
    "        V = V.real[:, idx[::-1]]\n",
    "        self.e = e\n",
    "        self.V = V\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y = None):\n",
    "        D = cdist(X, self.X, 'sqeuclidean')\n",
    "        E = np.exp(-D / (self._eps**2))\n",
    "        P = normalize(E, axis=1, norm='l1')\n",
    "        if(self._features is None):\n",
    "            return P.dot(self.V).dot(np.diag(1.0 / self.e))\n",
    "        else:\n",
    "            return P.dot(self.V).dot(np.diag(1.0 / self.e))[:, self._features]\n",
    "        \n",
    "    def __find_eps(self):\n",
    "        n = self.D.shape[0]\n",
    "        target = (1 / n) * self._target_std / 6;\n",
    "        i = 0\n",
    "        maxit = 100\n",
    "        a = self._max_eps\n",
    "        b = self._min_eps\n",
    "\n",
    "        P = normalize(np.exp(-self.D / (a**2)), axis=1, norm='l1')\n",
    "        fa = np.std(P.ravel())\n",
    "\n",
    "        P = normalize(np.exp(-self.D / (b**2)), axis=1, norm='l1')\n",
    "        fb = np.std(P.ravel())\n",
    "\n",
    "        while((abs(fa - fb) > (target/50)) and i < maxit):\n",
    "\n",
    "            c = (a+b)/2 \n",
    "            i = i + 1\n",
    "            P = normalize(np.exp(-self.D / (c**2)), axis=1, norm='l1')\n",
    "            fc = np.std(P.ravel())\n",
    "            if(fc < target):\n",
    "                a = c\n",
    "                fa = fc\n",
    "            else:\n",
    "                b = c\n",
    "                fb = fc\n",
    "        if(i >= maxit):\n",
    "            print(\"(Diffusion Maps)Reached max iterations without finding eps\")\n",
    "                \n",
    "        return c\n",
    "    \n",
    "    def getPOrdered(self):\n",
    "        idx = np.argsort(self.V[:,1])\n",
    "        P = self.P[:, idx]\n",
    "        P = P[idx, :]\n",
    "        return P\n",
    "    \n",
    "    def getP(self):\n",
    "        return self.P\n",
    "    \n",
    "    def hist(self):\n",
    "        y = np.sort(self.D.reshape(-1))\n",
    "        x = np.linspace(0,1, num=len(y))\n",
    "        plt.plot(x,y)\n",
    "        \n",
    "    def getE(self):\n",
    "        return self.e\n",
    "\n",
    "        \n",
    "        \n",
    "class Identity(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        return\n",
    "        \n",
    "    def fit(self, X, y = None ):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y = None):\n",
    "        return X\n",
    "\n",
    "def index2Date(index):\n",
    "    days=(31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31)\n",
    "    month=(\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\")\n",
    "    \n",
    "    xticks=np.roll(days, -5)\n",
    "    xticks_cum=np.cumsum(xticks)\n",
    "    xtlabels = np.roll(month, -5);\n",
    "    for i in range(0, len(xticks)):\n",
    "        total = xticks_cum[i]\n",
    "        day = xticks[i]\n",
    "        mon = xtlabels[i]\n",
    "        if(index < total):\n",
    "            return  mon + \" \" + str(1 + day - (total - index));\n",
    "    return \"error\";\n",
    "    \n",
    "def format_plot(ylim=True):\n",
    "    days=(31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31)\n",
    "    month=(\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\")\n",
    "    plt.xlabel(\"Date\", fontsize=22)\n",
    "    plt.ylabel(\"Score\", fontsize=22)\n",
    "    \n",
    "    xticks=np.roll(days, -4)\n",
    "    xticks[0]=5\n",
    "    xticks=np.cumsum(xticks)\n",
    "    xtlabels = np.roll(month, -5);\n",
    "    plt.xticks(xticks[0:9], xtlabels[0:9], fontsize=18)\n",
    "    plt.grid()\n",
    "    if(ylim):\n",
    "        plt.yticks(np.arange(0, 1.01, step=0.1), fontsize=18)\n",
    "        plt.ylim((0.0, 1.01))\n",
    "\n",
    "def plot_confidence(m, s, color):\n",
    "    x = np.linspace(1, len(m), len(m))\n",
    "    k=3\n",
    "    plt.fill_between(x, m+k*s, m-k*s, color=color+'33')\n",
    "    plt.plot(x, m, color=color,linewidth=3)\n",
    "    \n",
    "def chi2_distance(A, B): \n",
    "    chi = 0.5 * np.sum([((a - b) ** 2) / (a + b)  \n",
    "                      for (a, b) in zip(A, B)]) \n",
    "  \n",
    "    return chi \n",
    "\n",
    "def write_line(csv_file, data, name):\n",
    "    writer = csv.writer(csv_file, delimiter=',')\n",
    "    line = list(data)\n",
    "    line.insert(0, name)\n",
    "    writer.writerow(line)\n",
    "######################################\n",
    "# End of Utils\n",
    "######################################\n",
    "\n",
    "######################################\n",
    "# Begin of Grid Model Generation\n",
    "######################################\n",
    "\n",
    "\n",
    "#input files\n",
    "path = \"csv_data\";\n",
    "grid_path = \"grid\"\n",
    "temp_file = \"temp_avg.csv\"\n",
    "precip_file = \"precip.csv\"\n",
    "dengue_file = \"dengue.csv\"\n",
    "\n",
    "#Cities name\n",
    "states = glob(os.path.join(\".\", path, \"*/\"))\n",
    "##\n",
    "#Run for each city\n",
    "##\n",
    "for which_state in states:\n",
    "    ##\n",
    "    #Read csv files\n",
    "    ##\n",
    "    temp = pd.read_csv(os.path.join(which_state,temp_file))\n",
    "    precip = pd.read_csv(os.path.join(which_state,precip_file))\n",
    "    dengue = pd.read_csv(os.path.join(which_state,dengue_file))\n",
    "    ##\n",
    "    #get out of sample test data\n",
    "    ##\n",
    "    precip_out = precip.iloc[11:]\n",
    "    temp_out = temp.iloc[11:]\n",
    "    dengue_out = dengue.iloc[11:]\n",
    "    ##\n",
    "    #get train data\n",
    "    ##\n",
    "    precip = precip.iloc[0:11]\n",
    "    temp = temp.iloc[0:11]\n",
    "    dengue = dengue.iloc[0:11]\n",
    "\n",
    "    t_m = np.mean(temp.values)\n",
    "    t_s = np.std(temp.values)\n",
    "\n",
    "    p_m = np.mean(precip.values)\n",
    "    p_s = np.std(precip.values)\n",
    "    ##\n",
    "    #create output path\n",
    "    ##\n",
    "    state_name = which_state.split(os.sep)[2]\n",
    "    rpath = os.path.join(\".\", grid_path,state_name)\n",
    "    print(\"Grid search for \" + state_name + \"\\n\\n\\n\")\n",
    "    if not os.path.exists(os.path.dirname(rpath)):\n",
    "        os.makedirs(rpath)\n",
    "    ##\n",
    "    #Process the data\n",
    "    ##\n",
    "    outbreak_threshold = 100.0 #incidence threshold\n",
    "    x_temp = (temp.values - t_m) / t_s\n",
    "    x_precip = (precip.values - p_m) / p_s\n",
    "\n",
    "    y_dengue = np.zeros((len(dengue['incidence'])), dtype=int)\n",
    "    y_dengue[dengue['incidence'] >= outbreak_threshold] = 1\n",
    "\n",
    "    x_temp_out = (temp_out.values - t_m) / t_s\n",
    "    x_precip_out = (precip_out.values - p_m) / p_s\n",
    "\n",
    "    y_out = np.zeros((len(dengue_out['incidence'])), dtype=int)\n",
    "    y_out[dengue_out['incidence'] >= outbreak_threshold] = 1\n",
    "\n",
    "    #Data processing parameters\n",
    "    base = np.sqrt(2) #filter suport base\n",
    "    n_levels = 18 #number of levels to generate\n",
    "    x_t_train, x_p_train = process_data(x_temp, x_precip, base, n_levels)\n",
    "    x_t_out, x_p_out = process_data(x_temp_out, x_precip_out, base, n_levels)\n",
    "\n",
    "    ##\n",
    "    #Generation of validation noisy data\n",
    "    ##\n",
    "    n_rep = 2000 #number of noise data repetitions\n",
    "    precip_rep = np.repeat(precip.values, n_rep, axis=0)\n",
    "    temp_rep = np.repeat(temp.values, n_rep, axis=0)\n",
    "    y_rep = np.repeat(y_dengue, n_rep, axis=0)\n",
    "\n",
    "    size = precip_rep.shape\n",
    "\n",
    "    precip_noise = np.random.normal(0.0, 1.0, size=size)\n",
    "    temp_noise = np.random.normal(0.0, 1.0, size=size)\n",
    "    \n",
    "    \n",
    "    s = 2.5 #Noise intensity (sigma)\n",
    "    x_p_val = ((precip_rep + precip_noise*s) - p_m) / p_s\n",
    "    x_t_val = ((temp_rep + temp_noise*s) - t_m) / t_s\n",
    "    x_t_val, x_p_val = process_data(x_t_val, x_p_val, base, n_levels)\n",
    "\n",
    "    y_d_train = y_dengue\n",
    "    y_d_out = y_out\n",
    "    y_d_val = y_rep\n",
    "    \n",
    "    \n",
    "    \n",
    "    ##\n",
    "    # Grid Search\n",
    "    ##\n",
    "    %matplotlib agg\n",
    "    %matplotlib agg\n",
    "\n",
    "    colors = {'accuracy_val': '#7777FF',\n",
    "              'accuracy_test': '#FF7777', \n",
    "              'accuracy_train': '#77FF77',}\n",
    "\n",
    "    ##\n",
    "    #Grid parameters\n",
    "    ##\n",
    "    best =[(1, 4), (4, 7), (7, 10), (10, 13), (12, 15)] #Set of bands to use\n",
    "    bdays = [3, 5, 9, 12] #Set of windows (size) to use\n",
    "    nfeatures = [3] #Set of features to use (2,n), uses feature (2,3) since feature 1 is constant (Diffusion Maps)\n",
    "    epsilon = [\"auto\"] #Epsilon for DM, automatic\n",
    "\n",
    "    #For the set of bands\n",
    "    for k in range(0, len(bdays)):\n",
    "        n_days = bdays[k]\n",
    "        #For the set of windows\n",
    "        for j in range(0,len(best)):\n",
    "            l=best[j]\n",
    "            #For the set of features\n",
    "            for f in range(0,len(nfeatures)):\n",
    "                features=(1,nfeatures[f])\n",
    "                #For the set of epsilon\n",
    "                for e in range(0, len(epsilon)):\n",
    "                    eps = epsilon[e]\n",
    "                    fname = \"ndays.\"+str(n_days)+\"_range.\"+str(l)+\"_feat.\"+str(features)+\"_eps.\"+str(eps)\n",
    "                    print(\"N Days = \" + str(n_days) + \" Range = \" + str(l) + \" Features = \" + str(features) + \" Epsilon = \" + str(eps))\n",
    "\n",
    "                    model = Pipeline(steps =[(\"Scaler\", StandardScaler()),\n",
    "                                             (\"DM\", DiffusionMaps(features=features)),\n",
    "                                             (\"SVM\", SVC(C=10, gamma=1, kernel=\"rbf\"))]) \n",
    "\n",
    "                    start_time = timeit.default_timer()\n",
    "\n",
    "                    m_test_, m_val_, m_train_ = make_grid(x_t_train, x_t_val, x_t_out, x_p_train, x_p_val, x_p_out, y_d_train, y_d_val, y_d_out, n_days, l,  model)\n",
    "                    \n",
    "                    elapsed = timeit.default_timer() - start_time\n",
    "                    progress(s, s, \" Total time: \" + formatTime(elapsed))\n",
    "\n",
    "\n",
    "                    with open(os.path.join(rpath, fname+\".csv\"), \"w\") as csv_file:\n",
    "                        writer = csv.writer(csv_file, delimiter=',')\n",
    "                        for key in m_test_:\n",
    "                            line = list(m_test_[key])\n",
    "                            line.insert(0, str(key)+\"_test\")\n",
    "                            writer.writerow(line)\n",
    "                        for key in m_train_:\n",
    "                            line = list(m_train_[key])\n",
    "                            line.insert(0, str(key)+\"_train\")\n",
    "                            writer.writerow(line)\n",
    "                        for key in m_val_:\n",
    "                            line = list(m_val_[key])\n",
    "                            line.insert(0, str(key)+\"_val\")\n",
    "                            writer.writerow(line)\n",
    "\n",
    "\n",
    "                    fig = plt.figure(figsize=(20,10))\n",
    "                    acc = m_test_['accuracy']\n",
    "                    acc2 = m_train_['accuracy']\n",
    "                    acc3 = m_val_['accuracy']\n",
    "                    print(\"\\n count: \" + str(len(acc3[acc3 > 0.8])) + \n",
    "                             \" mean acc: \" + str(np.mean(acc3[acc3 > 0.8])) +\n",
    "                              \" max acc: \" + str(max(acc3)))\n",
    "\n",
    "                    plt.plot(acc, color=colors[\"accuracy_test\"], linewidth=4)\n",
    "                    plt.plot(acc2, color=colors[\"accuracy_train\"], linewidth=4)\n",
    "                    plt.plot(acc3, color=colors[\"accuracy_val\"], linewidth=4)\n",
    "                    plt.legend([\"Test acc.\", \"Train acc.\", \"Val. acc.\"], fontsize=22)\n",
    "                    format_plot()\n",
    "                    plt.show()\n",
    "                    fig.savefig(os.path.join(rpath, fname+\".png\"))\n",
    "                    plt.close()\n",
    "                    \n",
    "                    \n",
    "######################################\n",
    "# End of Grid Model Generation\n",
    "######################################\n",
    "\n",
    "######################################\n",
    "# Begin of Smoothing and Functionals\n",
    "######################################\n",
    "\n",
    "%matplotlib agg\n",
    "%matplotlib agg\n",
    "\n",
    "colors = {'accuracy_val': '#7777FF',\n",
    "          'accuracy_test': '#FF7777', \n",
    "          'accuracy_train': '#77FF77',}\n",
    "\n",
    "\n",
    "\n",
    "base_size = 15 #Smooth (convolution) filter size\n",
    "mean_filter = np.ones((base_size,)) * 1/base_size\n",
    "smoothed_path = \"smoothed\"\n",
    "\n",
    "#For each city\n",
    "for state in states:\n",
    "    state_name = state.split(os.sep)[2]\n",
    "    dirname = os.path.join(\".\", smoothed_path, state_name)\n",
    "    if not os.path.exists(os.path.dirname(dirname)):\n",
    "        os.makedirs(dirname)\n",
    "        time.sleep(1)\n",
    "        \n",
    "    #For each grid result\n",
    "    for file in glob(os.path.join(\".\", grid_path, state_name, \"*.csv\")):\n",
    "        ##\n",
    "        #Read data\n",
    "        ##\n",
    "        data = pd.read_csv(file, header=None, index_col=[0]).transpose()\n",
    "        filename = file.split(os.sep)[-1]\n",
    "\n",
    "        acc_test = data[\"accuracy_test\"].values\n",
    "        acc_val = data[\"accuracy_val\"].values\n",
    "        acc_train = data[\"accuracy_train\"].values\n",
    "\n",
    "        ##\n",
    "        #Smooth data\n",
    "        ##\n",
    "        mean_acc_test = np.convolve(acc_test, mean_filter, mode=\"same\")\n",
    "        mean_acc_val = np.convolve(acc_val, mean_filter, mode=\"same\")\n",
    "        mean_acc_train = np.convolve(acc_train, mean_filter, mode=\"same\")\n",
    "\n",
    "        acc_test_pad = np.pad(acc_test, base_size//2, mode=\"reflect\")\n",
    "        acc_val_pad = np.pad(acc_val, base_size//2, mode=\"reflect\")\n",
    "        acc_train_pad = np.pad(acc_train, base_size//2, mode=\"reflect\")\n",
    "\n",
    "        mean_acc_test_pad = np.pad(acc_test, base_size//2, mode=\"reflect\")\n",
    "        mean_acc_val_pad = np.pad(acc_val, base_size//2, mode=\"reflect\")\n",
    "        mean_acc_train_pad = np.pad(acc_train, base_size//2, mode=\"reflect\")\n",
    "\n",
    "        ##\n",
    "        #Calculate some other properties for each grid model\n",
    "        ##\n",
    "        vsize = len(mean_acc_test)\n",
    "        un_test = np.zeros((vsize,))\n",
    "        un_val = np.zeros((vsize,))\n",
    "        un_train = np.zeros((vsize,))\n",
    "\n",
    "        div_test = np.zeros((vsize,))\n",
    "        div_val = np.zeros((vsize,))\n",
    "\n",
    "        chi2_test = np.zeros((vsize,))\n",
    "        chi2_val = np.zeros((vsize,))\n",
    "\n",
    "        for i in range(0, vsize):\n",
    "            un_test[i] = np.mean((acc_test_pad[i:(i+base_size)] - mean_acc_test[i])**2)\n",
    "            un_val[i] = np.mean((acc_val_pad[i:(i+base_size)] - mean_acc_val[i])**2)\n",
    "            un_train[i] = np.mean((acc_train_pad[i:(i+base_size)] - mean_acc_train[i])**2)\n",
    "\n",
    "            div_test[i] = entropy(mean_acc_train_pad[i:(i+base_size)], mean_acc_test_pad[i:(i+base_size)])\n",
    "            div_val[i] = entropy(mean_acc_train_pad[i:(i+base_size)], mean_acc_val_pad[i:(i+base_size)])\n",
    "\n",
    "            chi2_test[i] = chi2_distance(mean_acc_train_pad[i:(i+base_size)], mean_acc_test_pad[i:(i+base_size)])\n",
    "            chi2_val[i] = chi2_distance(mean_acc_train_pad[i:(i+base_size)], mean_acc_val_pad[i:(i+base_size)])\n",
    "\n",
    "        un_test = np.sqrt(un_test)\n",
    "        un_val = np.sqrt(un_val)\n",
    "        un_train = np.sqrt(un_train)\n",
    "\n",
    "\n",
    "\n",
    "        ##\n",
    "        #Save results\n",
    "        ##\n",
    "        with open(os.path.join(\".\", smoothed_path, state_name, filename), \"w\") as csv_file:\n",
    "            write_line(csv_file, mean_acc_test, \"mean_accuracy_test\")\n",
    "            write_line(csv_file, mean_acc_val, \"mean_accuracy_val\")\n",
    "            write_line(csv_file, mean_acc_train, \"mean_accuracy_train\")\n",
    "            write_line(csv_file, acc_test, \"accuracy_test\")\n",
    "            write_line(csv_file, acc_val, \"accuracy_val\")\n",
    "            write_line(csv_file, acc_train, \"accuracy_train\")\n",
    "            write_line(csv_file, un_test, \"uncertainty_test\")\n",
    "            write_line(csv_file, un_val, \"uncertainty_val\")\n",
    "            write_line(csv_file, un_train, \"uncertainty_train\")\n",
    "            write_line(csv_file, div_val, \"divergence_val\")\n",
    "            write_line(csv_file, div_test, \"divergence_test\")\n",
    "            write_line(csv_file, chi2_val, \"chi2_val\")\n",
    "            write_line(csv_file, chi2_test, \"chi2_test\")\n",
    "\n",
    "\n",
    "\n",
    "        print(file)\n",
    "        fig = plt.figure(figsize=(20,10))\n",
    "        plot_confidence(mean_acc_test, un_test, color=colors[\"accuracy_test\"])\n",
    "        plot_confidence(mean_acc_train, un_train, color=colors[\"accuracy_train\"])\n",
    "        plot_confidence(mean_acc_val, un_val, color=colors[\"accuracy_val\"])\n",
    "        plt.legend([\"Test acc.\", \"Train acc.\", \"Val acc.\"])\n",
    "        format_plot()\n",
    "        plt.show()\n",
    "        fig.savefig(os.path.join(\".\", smoothed_path, state_name, filename[:-3]+\"png\"))\n",
    "        plt.close()\n",
    "      \n",
    "######################################\n",
    "# End of Smoothing and Functionals\n",
    "######################################\n",
    "\n",
    "######################################\n",
    "# Begin of Result Selection\n",
    "######################################\n",
    "\n",
    "base_size = 10 #distance between peaks to find in the data\n",
    "rmax = []\n",
    "rmean = []\n",
    "selection_path = \"selection\"\n",
    "\n",
    "result_data = pd.DataFrame(columns=[\"city\", \"max_acc_test\", \"mean_acc_test\", \n",
    "                                       \"max_acc_train\", \"mean_acc_train\", \n",
    "                                       \"max_acc_val\", \"mean_acc_val\",\n",
    "                                       \"uncertainty\", \"divergence\", \"diff\", \"chi2\",\n",
    "                                       \"index\", \"date\", \"file\", \"sum\"])\n",
    "\n",
    "#For each city\n",
    "for state in states:\n",
    "    state_name = state.split(os.sep)[2]\n",
    "    mean_score = pd.DataFrame(columns=[\"max_acc_test\", \"mean_acc_test\", \n",
    "                                       \"max_acc_train\", \"mean_acc_train\", \n",
    "                                       \"max_acc_val\", \"mean_acc_val\",\n",
    "                                       \"uncertainty\", \"divergence\", \"diff\", \"chi2\",\n",
    "                                       \"index\", \"date\", \"file\"])\n",
    "    max_score = pd.DataFrame(columns=[\"max_acc_test\", \"mean_acc_test\", \n",
    "                                      \"max_acc_train\", \"mean_acc_train\", \n",
    "                                      \"max_acc_val\", \"mean_acc_val\",\n",
    "                                      \"uncertainty\", \"divergence\", \"diff\", \"chi2\",\n",
    "                                      \"index\", \"date\", \"file\"])\n",
    "\n",
    "\n",
    "    #For each grid model\n",
    "    for file in glob(os.path.join(\".\", smoothed_path, state_name, \"*.csv\")):\n",
    "        data = pd.read_csv(file, header=None, index_col=[0]).transpose()\n",
    "\n",
    "        ##\n",
    "        #Read the data\n",
    "        ##\n",
    "        acc_test = data[\"accuracy_test\"].values\n",
    "        acc_val = data[\"accuracy_val\"].values\n",
    "        acc_train = data[\"accuracy_train\"].values\n",
    "        uncertainty = data[\"uncertainty_val\"].values\n",
    "        divergence = data[\"divergence_val\"].values * 100\n",
    "        chi2 = data[\"chi2_val\"].values\n",
    "        mean_acc_test = data[\"mean_accuracy_test\"].values\n",
    "        mean_acc_val = data[\"mean_accuracy_val\"].values\n",
    "        mean_acc_train = data[\"mean_accuracy_train\"].values\n",
    "\n",
    "        ##\n",
    "        #Find peaks based on mean accuracy\n",
    "        ##\n",
    "        idx_mean, _ = find_peaks(mean_acc_val, distance=base_size//2)\n",
    "        for i_mean in idx_mean:\n",
    "            mean_score = mean_score.append({  \"max_acc_test\": acc_test[i_mean], \n",
    "                                   \"mean_acc_test\": mean_acc_test[i_mean],\n",
    "                                   \"max_acc_train\": acc_train[i_mean], \n",
    "                                   \"mean_acc_train\": mean_acc_train[i_mean],\n",
    "                                   \"max_acc_val\": acc_val[i_mean], \n",
    "                                   \"mean_acc_val\": mean_acc_val[i_mean],\n",
    "                                   \"uncertainty\": uncertainty[i_mean],\n",
    "                                   \"divergence\": divergence[i_mean],\n",
    "                                   \"diff\": acc_train[i_mean]-acc_val[i_mean],\n",
    "                                   \"chi2\": chi2[i_mean],\n",
    "                                   \"index\": i_mean, \n",
    "                                   \"date\": index2Date(i_mean),\n",
    "                                   \"file\": file.split(os.sep)[-1]}, ignore_index=True)\n",
    "        ##\n",
    "        #Find peaks based on accuracy\n",
    "        ##\n",
    "        idx_max, _ = find_peaks(acc_val, distance=base_size//2)\n",
    "        for i_max in idx_max:\n",
    "            max_score = mean_score.append({  \"max_acc_test\": acc_test[i_max], \n",
    "                                   \"mean_acc_test\": mean_acc_test[i_max],\n",
    "                                   \"max_acc_train\": acc_train[i_max], \n",
    "                                   \"mean_acc_train\": mean_acc_train[i_max],\n",
    "                                   \"max_acc_val\": acc_val[i_max], \n",
    "                                   \"mean_acc_val\": mean_acc_val[i_max],\n",
    "                                   \"uncertainty\": uncertainty[i_max],\n",
    "                                   \"divergence\": divergence[i_max],\n",
    "                                   \"diff\": acc_train[i_max]-acc_val[i_max],\n",
    "                                   \"chi2\": chi2[i_max],\n",
    "                                   \"index\": i_max, \n",
    "                                   \"date\": index2Date(i_max),\n",
    "                                   \"file\": file.split(os.sep)[-1]}, ignore_index=True)\n",
    "\n",
    "    max_score[\"sum\"] = (max_score[\"mean_acc_val\"]+max_score[\"mean_acc_train\"])/2\n",
    "    max_score = max_score.sort_values([\"sum\"], ascending=False)\n",
    "    \n",
    "    mean_score[\"sum\"] = (mean_score[\"mean_acc_val\"]+mean_score[\"mean_acc_train\"])/2\n",
    "    mean_score = mean_score.sort_values([\"sum\"], ascending=False)\n",
    "    \n",
    "    rmax.append(max_score)\n",
    "    rmean.append(mean_score)\n",
    "    if(max_score.size > 0):\n",
    "        line = max_score.iloc[0];\n",
    "        line[\"city\"] = state_name\n",
    "        result_data = result_data.append(line)\n",
    "        print(\"\\n\\n\"+state_name+\"\\n\\n\")\n",
    "        print(line)\n",
    "\n",
    "    dirname = os.path.join(\".\", selection_path, state_name)\n",
    "    if not os.path.exists(os.path.dirname(dirname)):\n",
    "        os.makedirs(dirname)\n",
    "\n",
    "    max_score.to_csv(os.path.join(dirname,\"max_\"+folder+\"_\"+str(base_size)+\".csv\"), float_format=\"%.2f\", index=False)\n",
    "    mean_score.to_csv(os.path.join(dirname,\"\\\\mean_\"+folder+\"_\"+str(base_size)+\".csv\"), float_format=\"%.2f\", index=False)\n",
    "\n",
    "######################################\n",
    "# End of Result Selection\n",
    "######################################\n",
    "\n",
    "######################################\n",
    "# Begin Save Final Result\n",
    "######################################\n",
    "\n",
    "result_path = \"result\"\n",
    "dirname = os.path.join(\".\", result_path)\n",
    "if not os.path.exists(os.path.dirname(dirname)):\n",
    "    os.makedirs(dirname)\n",
    "result_data.to_csv(os.path.join(dirname, \"result.csv\"), float_format=\"%.2f\", index=False)\n",
    "######################################\n",
    "# End Save Final Result\n",
    "######################################\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "301f591c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['./csv_data/Aracaju/',\n",
       "  './csv_data/BeloHorizonte/',\n",
       "  './csv_data/Manaus/',\n",
       "  './csv_data/Recife/',\n",
       "  './csv_data/RioDeJaneiro/',\n",
       "  './csv_data/Salvador/',\n",
       "  './csv_data/SaoLuis/'],\n",
       " '/')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"csv_data\";\n",
    "grid_path = \"grid\"\n",
    "temp_file = \"temp_avg.csv\"\n",
    "precip_file = \"precip.csv\"\n",
    "dengue_file = \"dengue.csv\"\n",
    "\n",
    "#Cities name\n",
    "states = glob(os.path.join(\".\", path, \"*/\"))\n",
    "states, os.sep"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
